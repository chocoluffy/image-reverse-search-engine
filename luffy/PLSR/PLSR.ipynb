{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cross_decomposition import PLSRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "Failed to interpret file './features/descriptions_train_concat_embed_vectors.npy' as a pickle",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-a883aa15df75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdes_concat_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./features/descriptions_train_concat_embed_vectors.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdes_concat_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./features/descriptions_test_concat_embed_vectors.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdes_long_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./features/descriptions_train_long_embed_vectors.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdes_long_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./features/descriptions_test_long_embed_vectors.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mimage_1000_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./features/image_features_1000_train.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/py27/lib/python2.7/site-packages/numpy/lib/npyio.pyc\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    441\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m                 raise IOError(\n\u001b[0;32m--> 443\u001b[0;31m                     \"Failed to interpret file %s as a pickle\" % repr(file))\n\u001b[0m\u001b[1;32m    444\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mown_fid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: Failed to interpret file './features/descriptions_train_concat_embed_vectors.npy' as a pickle"
     ]
    }
   ],
   "source": [
    "des_concat_train = np.load('./features/descriptions_train_concat_embed_vectors.npy')\n",
    "des_concat_test = np.load('./features/descriptions_test_concat_embed_vectors.npy')\n",
    "des_long_train = np.load('./features/descriptions_train_long_embed_vectors.npy')\n",
    "des_long_test = np.load('./features/descriptions_test_long_embed_vectors.npy')\n",
    "image_1000_train = np.load('./features/image_features_1000_train.npy')\n",
    "image_1000_test = np.load('./features/image_features_1000_test.npy')\n",
    "image_2048_train = np.load('./features/image_features_2048_train.npy')\n",
    "image_2048_test = np.load('./features/image_features_2048_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nearest_20(vec, vec_set):\n",
    "    dis = [np.linalg.norm(np.array(vec) - np.array(x)) for x in vec_set]\n",
    "    return np.argsort(dis)[:20]\n",
    "\n",
    "def map20score(y, pred):\n",
    "    if y in pred:\n",
    "        return (20 - pred.tolist().index(y)) / 20.0\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def map20eval(pls, des_vectors, image_vectors):\n",
    "    image_pred = pls.predict(des_vectors)\n",
    "    top_20 = [get_nearest_20(vec, image_vectors) for vec in image_pred]\n",
    "    print(len(top_20))\n",
    "    scores = [map20score(i, top_20[i]) for i in range(len(top_20))]\n",
    "    print \"score: %f\" % np.mean(scores)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/sklearn/cross_decomposition/pls_.py:77: UserWarning: Maximum number of iterations reached\n",
      "  warnings.warn('Maximum number of iterations reached')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PLSRegression(copy=True, max_iter=500, n_components=256, scale=True,\n",
       "       tol=1e-06)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pls_concat_1000 = PLSRegression(n_components=256)\n",
    "pls_concat_1000.fit(des_concat_train, image_1000_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_concat_1000 = pls_concat_1000.predict(des_concat_test)\n",
    "top_20_concat_1000 = [get_nearest_20(vec, image_1000_test) for vec in pred_concat_1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_submission(top_20, output_path):\n",
    "    top_20_image_IDs = map(lambda x: ' '.join(map(lambda xx: str(xx) + '.jpg', x)), top_20)\n",
    "    description_ID = map(lambda x: str(x) + '.txt', range(len(top_20)))\n",
    "    submission_df = pd.DataFrame({'Descritpion_ID': description_ID, 'Top_20_Image_IDs': top_20_image_IDs})\n",
    "    submission_df.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_submission(top_20_concat_1000, 'submissions/PLSR_256_concat_1000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PLSRegression(copy=True, max_iter=1000, n_components=256, scale=True,\n",
       "       tol=1e-06)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pls_concat_1000 = PLSRegression(n_components=256, max_iter=1000)\n",
    "pls_concat_1000.fit(des_concat_train, image_1000_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_concat_1000 = pls_concat_1000.predict(des_concat_test)\n",
    "top_20_concat_1000 = [get_nearest_20(vec, image_1000_test) for vec in pred_concat_1000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_submission(top_20_concat_1000, 'submissions/PLSR_256_concat_1000_1000iter.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "des_BOW_noun_train = np.load('features/descriptions_train_BOW_noun.npy')\n",
    "des_BOW_noun_test = np.load('features/descriptions_test_BOW_noun.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3791097044896606\n"
     ]
    }
   ],
   "source": [
    "pls_BOW_noun_1000 = PLSRegression(n_components=10)\n",
    "pls_BOW_noun_1000.fit(des_BOW_noun_train[:8000], image_1000_train[:8000])\n",
    "print pls_BOW_noun_1000.score(des_BOW_noun_train[8000:], image_1000_train[8000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "score: 0.290600\n"
     ]
    }
   ],
   "source": [
    "scores = map20eval(pls_BOW_noun_1000, des_BOW_noun_train[8000:], image_1000_train[8000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PLSRegression(copy=True, max_iter=500, n_components=256, scale=True,\n",
       "       tol=1e-06)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pls_BOW_noun_1000 = PLSRegression(n_components=256)\n",
    "pls_BOW_noun_1000.fit(des_BOW_noun_train, image_1000_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_BOW_noun_1000 = pls_BOW_noun_1000.predict(des_BOW_noun_test)\n",
    "top_20_BOW_noun_1000 = [get_nearest_20(vec, image_1000_test) for vec in pred_BOW_noun_1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_submission(top_20_BOW_noun_1000, 'submissions/PLSR_256_BOW_noun_1000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "des_BOW_noun_train = np.load('features/descriptions_train_BOW_noun_5452.npy')\n",
    "des_BOW_noun_test = np.load('features/descriptions_test_BOW_noun_5452.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=512, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.fit(des_BOW_noun_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "des_BOW_noun_train_pca = pca.transform(des_BOW_noun_train)\n",
    "des_BOW_noun_test_pca = pca.transform(des_BOW_noun_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 512)\n"
     ]
    }
   ],
   "source": [
    "print des_BOW_noun_train_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.43198989225672163\n"
     ]
    }
   ],
   "source": [
    "pls_BOW_noun_1000 = PLSRegression(n_components=20)\n",
    "pls_BOW_noun_1000.fit(des_BOW_noun_train_pca[:8000], image_1000_train[:8000])\n",
    "print pls_BOW_noun_1000.score(des_BOW_noun_train_pca[8000:], image_1000_train[8000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PLSRegression(copy=True, max_iter=500, n_components=256, scale=True,\n",
       "       tol=1e-06)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pls_BOW_noun_1000 = PLSRegression(n_components=256)\n",
    "pls_BOW_noun_1000.fit(des_BOW_noun_train_pca, image_1000_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_BOW_noun_1000_5452 = pls_BOW_noun_1000.predict(des_BOW_noun_test_pca)\n",
    "top_20_BOW_noun_1000_5452 = [get_nearest_20(vec, image_1000_test) for vec in pred_BOW_noun_1000_5452]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_submission(top_20_BOW_noun_1000_5452452, 'submissions/PLSR_256_BOW_noun_1000_5452_pca_512.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/envs/py27/lib/python2.7/site-packages/sklearn/base.py:251: UserWarning: Trying to unpickle estimator PCA from version 0.19.1 when using version 0.20.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 1024)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Luffy part.\n",
    "\"\"\"\n",
    "\n",
    "import pickle\n",
    "import os.path\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "train_description_feature_map = pickle.load(open('./features/11_30_[train]_description_normalized_only_>1_vector_py27.pkl', 'rb'))\n",
    "test_derscription_feature_map = pickle.load(open('./features/11_30_[test]_description_normalized_only_>1_vector_py27.pkl', 'rb'))\n",
    "\n",
    "train_bow_lst = list(map(lambda x: x[\"BOW_all_normalized_vector\"], train_description_feature_map))\n",
    "test_bow_lst = list(map(lambda x: x[\"BOW_all_normalized_vector\"], test_derscription_feature_map))\n",
    "\n",
    "pca_model_name = \"./models/pca_bow_all_to_1024.pkl\"\n",
    "if os.path.exists(pca_model_name):\n",
    "    pca = pickle.load(open(pca_model_name, 'rb'))\n",
    "else:\n",
    "    pca = PCA(n_components=3500)\n",
    "    pca.fit(train_bow_lst)\n",
    "    pickle.dump(pca, open(pca_model_name, 'wb'), protocol=2)\n",
    "\n",
    "des_BOW_all_train_pca = pca.transform(train_bow_lst)\n",
    "des_BOW_all_test_pca = pca.transform(test_bow_lst)\n",
    "\n",
    "print des_BOW_all_test_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_map = pickle.load(open('./features/12_1_train_img_feature_map_py27.pkl', 'rb'))\n",
    "test_img_map = pickle.load(open('./features/12_1_test_img_feature_map_py27.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 2048)\n"
     ]
    }
   ],
   "source": [
    "train_pool5_img = np.asarray(list(map(lambda x: x[\"POOL_vector\"], train_img_map)))\n",
    "test_pool5_img = np.asarray(list(map(lambda x: x[\"POOL_vector\"], test_img_map)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/envs/py27/lib/python2.7/site-packages/sklearn/base.py:251: UserWarning: Trying to unpickle estimator PLSRegression from version 0.19.1 when using version 0.20.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "model_name = \"./models/pls_600_pool5_to_BOW_all_pca_1024.pkl\"\n",
    "if os.path.exists(model_name):\n",
    "    model = pickle.load(open(model_name, 'rb'))\n",
    "else:\n",
    "    model = PLSRegression(n_components=800)\n",
    "    model.fit(train_pool5_img[:8000], des_BOW_all_train_pca[:8000])\n",
    "    pickle.dump(model, open(model_name, 'wb'), protocol=2)\n",
    "    print model.score(train_pool5_img[8000:], des_BOW_all_train_pca[8000:])\n",
    "print \"model loaded...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('description * images dist matrix, shape:', (2000, 2000))\n",
      "Writing Complete.\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "import csv\n",
    "\n",
    "pred_test_pool5 = model.predict(test_pool5_img)\n",
    "\n",
    "dist = cdist(des_BOW_all_test_pca, pred_test_pool5, 'cosine')\n",
    "print(\"description * images dist matrix, shape:\", dist.shape)\n",
    "sorted_id = np.argsort(dist) # dist: N_description * N_images dist matrix.\n",
    "\n",
    "with open('./submission/submission_plsr_600_pool5_to_bow_all_pca_1024.csv', 'w') as csvfile:\n",
    "        # write csv header\n",
    "        fieldnames = ['Descritpion_ID', \"Top_20_Image_IDs\"]\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        \n",
    "        for i, row in enumerate(sorted_id):\n",
    "            top_choices =  list(map(lambda x: str(x) + \".jpg\", row[:20]))\n",
    "            res = {}\n",
    "            res['Descritpion_ID'] = str(i) + \".txt\" # file name\n",
    "            res['Top_20_Image_IDs'] = \" \".join(top_choices)\n",
    "            writer.writerow(res)\n",
    "\n",
    "print(\"Writing Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
