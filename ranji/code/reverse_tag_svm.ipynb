{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "path_prefix = \"./all/data/\"\n",
    "word_to_index = {} # map word to index\n",
    "g_word_list = [] # sorted by frequence low to high\n",
    "total_word_dict = {}\n",
    "skip_words = set(['', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'as', 'an', 'and', 'are', 'at', 'by', 'down', 'for', 'from', 'front', 'has', 'in', 'is', 'it', 'near', 'next', 'of', 'on', 'out', 'over', 'several', 'sits', 'some', 'that', 'the', 'there', 'through', 'to', 'top', 'up', 'while', 'with'])\n",
    "train_des = [] #0/1/2 array of word\n",
    "test_des = [] #0/1/2 array of word\n",
    "train_feature_list = [] # list of feature_array by index\n",
    "test_feature_list = []\n",
    "all_predict_test_des = []\n",
    "train_g_category_tag = []\n",
    "train_g_item_tag = []\n",
    "test_g_category_tag = []\n",
    "test_g_item_tag = []\n",
    "MAX_INPUT = 100\n",
    "MAX_OUTPUT = 20\n",
    "COMPONENTS = 400 # need to run again\n",
    "PCA_NUMBER = 2000 # need to run again\n",
    "ITER = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_to_string(file_name, keep_colon):\n",
    "    f = open(file_name, \"r\")\n",
    "    content = f.read()\n",
    "    f.close()\n",
    "    content = content.lower()\n",
    "    content = content.replace(\",\", \" \").replace(\".\", \" \").replace(\"'\", \" \").replace(\"\\\"\", \" \")\n",
    "    if (keep_colon == False):\n",
    "        content = content.replace(\":\", \" \")\n",
    "    return content\n",
    "\n",
    "def read_file_to_total_word_dict(file_name, keep_colon):\n",
    "    word_list = read_file_to_string(file_name, keep_colon).split()\n",
    "    for name in word_list:\n",
    "        if name in skip_words:\n",
    "            continue\n",
    "        if name in total_word_dict:\n",
    "            total_word_dict[name] += 1\n",
    "            continue\n",
    "        else:\n",
    "            total_word_dict[name] = 1\n",
    "            continue\n",
    "\n",
    "def print_word_in_array(a):\n",
    "    ret = \"\"\n",
    "    for i in range(len(a)):\n",
    "        if a[i] != 0:\n",
    "            ret = ret + str(a[i]) + g_word_list[i] + \" \"\n",
    "    print(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_word_dict = {}\n",
    "for i in range(MAX_INPUT):\n",
    "    read_file_to_total_word_dict(path_prefix+\"descriptions_train/\"+str(i)+\".txt\", False)\n",
    "    read_file_to_total_word_dict(path_prefix+\"tags_train/\"+str(i)+\".txt\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "def cmp(a, b):\n",
    "    a_num = abs(float(a.split(\"___\")[0]))\n",
    "    b_num = abs(float(b.split(\"___\")[0]))\n",
    "    if (a_num < b_num):\n",
    "        return -1\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load g_word_list\n",
    "g_word_list = []\n",
    "word_to_index = {}\n",
    "tmp_list = []\n",
    "#TODO: maybe try filter on number\n",
    "for word in total_word_dict:\n",
    "    if total_word_dict[word] <= 0:\n",
    "        continue\n",
    "    tmp_list.append(str(total_word_dict[word])+\"___\"+word)\n",
    "\n",
    "tmp_list = sorted(tmp_list, key=functools.cmp_to_key(cmp))\n",
    "\n",
    "for i in range(len(tmp_list)):\n",
    "    word = tmp_list[i].split(\"___\")[1]\n",
    "    g_word_list.append(word)\n",
    "    word_to_index[word] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read to tags\n",
    "train_g_category_tag = [0] * MAX_INPUT\n",
    "train_g_item_tag = [0] * MAX_INPUT\n",
    "test_g_category_tag = [0] * MAX_OUTPUT\n",
    "test_g_item_tag = [0] * MAX_OUTPUT\n",
    "\n",
    "#TODO: add tags in descriptions\n",
    "def is_in_tag(file_number, word, train):\n",
    "    if train:\n",
    "        return (word in train_g_category_tag[file_number]) or (word in train_g_item_tag[file_number])\n",
    "    return (word in test_g_category_tag[file_number]) or (word in test_g_item_tag[file_number])\n",
    "\n",
    "def read_tags(file_name, train):\n",
    "    lines = read_file_to_string(file_name, True).split(\"\\n\")\n",
    "    index = int(file_name.split(\"/\")[-1].replace(\".txt\", \"\"))\n",
    "    current_category_set = set([])\n",
    "    current_item_set = set([])\n",
    "    \n",
    "    for line in lines:\n",
    "        cat = line.split(\":\")[0].strip()\n",
    "        if cat in word_to_index:\n",
    "            current_category_set.add(cat)\n",
    "\n",
    "        item = ''\n",
    "        if (cat.strip() != line.replace(\":\", \"\").strip()):\n",
    "            item = line.split(\":\")[1].strip()\n",
    "        if item in word_to_index:\n",
    "            current_item_set.add(item)\n",
    "    if (train):\n",
    "        train_g_category_tag[index] = current_category_set\n",
    "        train_g_item_tag[index] = current_item_set\n",
    "    else:\n",
    "        test_g_category_tag[index] = current_category_set\n",
    "        test_g_item_tag[index] = current_item_set\n",
    "    \n",
    "\n",
    "for i in range(MAX_INPUT):\n",
    "    read_tags(path_prefix+\"tags_train/\"+str(i)+\".txt\", True)\n",
    "for i in range(MAX_OUTPUT):\n",
    "    read_tags(path_prefix+\"tags_test/\"+str(i)+\".txt\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where to make the change\n",
    "# todo: remove count to match test output\n",
    "# lenpi ci, yaoshi you dehua, jianfen\n",
    "# also try norm\n",
    "\n",
    "# count_max_weight = 2.0\n",
    "# uniqueness_max_weight = 2.0\n",
    "# TODO: add words in tag_item to pic\n",
    "def my_weight_func(index, count, train, file_num):\n",
    "    if (count <= 0):\n",
    "        return 0\n",
    "#     count_apply = 0\n",
    "#     if is_in_tag(file_num, g_word_list[index], train):\n",
    "#         count_apply = 1\n",
    "#     elif count >= 3:\n",
    "#         count_apply = 1\n",
    "#     return 1 + (len(g_word_list)-index)*uniqueness_max_weight/len(g_word_list) + count_apply*count_max_weight\n",
    "\n",
    "#     if count >= 5:\n",
    "#         return count\n",
    "#     if is_in_tag(file_num, g_word_list[index], train):\n",
    "#         return 5\n",
    "    return 1\n",
    "\n",
    "def weight_occurance_array(occurance_array, train, file_num):\n",
    "    ret = [0] * len(occurance_array)\n",
    "    for index in range(len(occurance_array)):\n",
    "        ret[index] = my_weight_func(index, occurance_array[index], train, file_num)\n",
    "    return ret\n",
    "\n",
    "def read_des_to_list(path, file_num):\n",
    "    ret = []\n",
    "    train = (path.find(\"train\") != -1)\n",
    "    for i in range(file_num):\n",
    "        int_array = [0] * len(g_word_list)\n",
    "        temp_list = read_file_to_string(path+str(i)+\".txt\", False).split()\n",
    "        for word in temp_list:\n",
    "            if not word in word_to_index:\n",
    "                continue\n",
    "            int_array[word_to_index[word]] += 1\n",
    "        ret.append(weight_occurance_array(int_array, train, i))\n",
    "    return ret\n",
    "\n",
    "train_des = read_des_to_list(path_prefix+\"descriptions_train/\", MAX_INPUT)\n",
    "test_des = read_des_to_list(path_prefix+\"descriptions_test/\", MAX_OUTPUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add tag to desc list\n",
    "\n",
    "#TODO: careful with tag number\n",
    "def add_tag_to_train_des():\n",
    "    for sentence_index in range(MAX_INPUT):\n",
    "        array_to_add = [0] * len(g_word_list)\n",
    "        for tag in train_g_item_tag[sentence_index]:\n",
    "            array_to_add[word_to_index[tag]] = 1\n",
    "        \n",
    "        ret = train_des[sentence_index].copy()\n",
    "        for word_index in range(len(g_word_list)):\n",
    "            ret[word_index] = max(ret[word_index], array_to_add[word_index])\n",
    "        train_des[sentence_index] = ret\n",
    "\n",
    "def add_tag_to_test_des():\n",
    "    for sentence_index in range(MAX_OUTPUT):\n",
    "        array_to_add = [0] * len(g_word_list)\n",
    "        for tag in test_g_item_tag[sentence_index]:\n",
    "            array_to_add[word_to_index[tag]] = 1\n",
    "        \n",
    "        ret = all_predict_test_des[sentence_index].copy()\n",
    "        print(all_predict_test_des[sentence_index])\n",
    "        for word_index in range(len(g_word_list)):\n",
    "            ret[word_index] = max(ret[word_index], array_to_add[word_index])\n",
    "        all_predict_test_des[sentence_index] = ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_tag_to_train_des()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load feature list\n",
    "def load_feature_into_list(filename):\n",
    "    f = open(filename, \"r\")\n",
    "    content = f.readlines()\n",
    "    f.close()\n",
    "    train = filename.find(\"train\")\n",
    "    for line in content:\n",
    "        name_index = int(line.split(\".jpg,\")[0].split(\"/\")[1])\n",
    "        feature_list = line.split(\".jpg,\")[1].split(\",\")\n",
    "        if train != -1:\n",
    "            if name_index >= MAX_INPUT:\n",
    "                continue\n",
    "            train_feature_list[name_index] = feature_list\n",
    "        else:\n",
    "            if name_index >= MAX_OUTPUT:\n",
    "                continue\n",
    "            test_feature_list[name_index] = feature_list\n",
    "\n",
    "train_feature_list = [0] * MAX_INPUT\n",
    "test_feature_list = [0] * MAX_OUTPUT\n",
    "# load_feature_into_list(path_prefix+\"features_train/features_resnet1000_train.csv\")\n",
    "# load_feature_into_list(path_prefix+\"features_test/features_resnet1000_test.csv\")\n",
    "load_feature_into_list(path_prefix+\"features_train/features_resnet1000intermediate_train.csv\")\n",
    "load_feature_into_list(path_prefix+\"features_test/features_resnet1000intermediate_test.csv\")\n",
    "\n",
    "train_feature_list = np.asarray(train_feature_list, dtype=np.float64)\n",
    "test_feature_list = np.asarray(test_feature_list, dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 1001)\n",
      "(20, 1001)\n",
      "(100, 2048)\n",
      "(20, 2048)\n",
      "Done 0\n",
      "Done 1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "bad input shape (100, 1001)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-6cb32ee766aa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLinearSVC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Done 1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_feature_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_des_pca\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Done 2\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m         X, y = check_X_y(X, y, accept_sparse='csr',\n\u001b[1;32m--> 227\u001b[1;33m                          dtype=np.float64, order=\"C\")\n\u001b[0m\u001b[0;32m    228\u001b[0m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    576\u001b[0m                         dtype=None)\n\u001b[0;32m    577\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 578\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    579\u001b[0m         \u001b[0m_assert_all_finite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    580\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_numeric\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'O'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcolumn_or_1d\u001b[1;34m(y, warn)\u001b[0m\n\u001b[0;32m    612\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    613\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 614\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"bad input shape {0}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    615\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: bad input shape (100, 1001)"
     ]
    }
   ],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "# pca = PCA(n_components = PCA_NUMBER)\n",
    "# pca.fit(train_des)\n",
    "# train_des_pca = pca.transform(train_des)\n",
    "# test_des_pca = pca.transform(test_des)\n",
    "train_des_pca = np.asarray(train_des)\n",
    "test_des_pca = np.asarray(test_des)\n",
    "\n",
    "print(train_des_pca.shape)\n",
    "print(test_des_pca.shape)\n",
    "\n",
    "# pca2 = PCA(n_components = PCA_NUMBER)\n",
    "# pca2.fit(train_feature_list)\n",
    "# train_feature_list = pca2.transform(train_feature_list)\n",
    "# test_feature_list = pca2.transform(test_feature_list)\n",
    "train_feature_list = np.asarray(train_feature_list)\n",
    "test_feature_list = np.asarray(test_feature_list)\n",
    "\n",
    "print(train_feature_list.shape)\n",
    "print(test_feature_list.shape)\n",
    "\n",
    "# from sklearn.cross_decomposition import PLSRegression\n",
    "# print(\"Done 0\")\n",
    "# model = PLSRegression(COMPONENTS, max_iter=ITER)\n",
    "# print(\"Done 1\")\n",
    "# model.fit(train_feature_list, train_des_pca)\n",
    "# print(\"Done 2\")\n",
    "\n",
    "\n",
    "from sklearn import svm\n",
    "print(\"Done 0\")\n",
    "clf = svm.SVC()\n",
    "print(\"Done 1\")\n",
    "clf.fit(train_feature_list, train_des_pca)\n",
    "print(\"Done 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Start 3\")\n",
    "all_predict_test_des = clf.predict(test_feature_list)\n",
    "print(\"Done 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_tag_to_test_des()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "\n",
    "# return a score\n",
    "def compare_feature(target, given):\n",
    "    return float(sklearn.metrics.pairwise.cosine_distances([np.array(target, dtype=float)], [np.array(given, dtype=float)]))\n",
    "#     return float(np.linalg.norm(np.array(target, dtype=float) - np.array(given, dtype=float)))\n",
    "\n",
    "def get_top_20(target):\n",
    "    ret = []\n",
    "    for index in range(MAX_OUTPUT):\n",
    "        given = all_predict_test_des[index]\n",
    "        score = compare_feature(target, given)\n",
    "        item = str(score) + \"___\" + str(index)\n",
    "        ret.append(item)\n",
    "    return sorted(ret, key=functools.cmp_to_key(cmp))[:20]\n",
    "\n",
    "# f = open(\"reverse_count_\"+str(count_max_weight)+\"unique\"+str(uniqueness_max_weight)+\".csv\", \"w\")\n",
    "f = open(\"reverse_nocount_tag_svm.csv\", \"w\")\n",
    "f.write(\"Descritpion_ID,Top_20_Image_IDs\\n\")\n",
    "for index in range(MAX_OUTPUT):\n",
    "    target_desc = test_des_pca[index]\n",
    "    twenty_image_list = get_top_20(target_desc)\n",
    "    string_to_write = \"\" + str(index) + \".txt,\"\n",
    "    for name in twenty_image_list:\n",
    "        file_name = name.split(\"___\")[1] + \".jpg\"+\" \"\n",
    "        string_to_write += file_name\n",
    "    string_to_write = string_to_write.strip() + \"\\n\"\n",
    "    print(string_to_write)\n",
    "    f.write(string_to_write)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
